{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_meteo= pd.read_csv(\"concat_meteo.csv\")\n",
    "dif= pd.read_csv(\"frequentation_actu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fichier_concat_good1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mfichier_concat_good1.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Enlever les guillemets de chaque ligne\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/api/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fichier_concat_good1.csv'"
     ]
    }
   ],
   "source": [
    "with open('fichier_concat_good1.csv', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Enlever les guillemets de chaque ligne\n",
    "for i in range(len(lines)):\n",
    "    lines[i] = lines[i].replace('\"', '')\n",
    "\n",
    "# Écrire les lignes modifiées dans un nouveau fichier\n",
    "with open('fichier_concat_good1.csv', 'w') as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cibler les colonnes des csv meteo utiles à notre analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DATE  PRECIP_TOTAL_DAY_MM  CLOUDCOVER_AVG_PERCENT  SUNHOUR  \\\n",
      "0  2021-01-01                  0.5                  82.625      3.2   \n",
      "1  2021-01-02                  0.1                  54.750      8.6   \n",
      "2  2021-01-03                  0.3                  71.375      8.6   \n",
      "3  2021-01-04                  1.0                 100.000      3.2   \n",
      "4  2021-01-05                  0.4                  98.375      3.2   \n",
      "\n",
      "   VISIBILITY_AVG_KM  \n",
      "0              7.750  \n",
      "1             10.000  \n",
      "2             10.000  \n",
      "3              6.875  \n",
      "4              7.750  \n"
     ]
    }
   ],
   "source": [
    "df_meteo = df_meteo.loc[:, ['DATE', 'PRECIP_TOTAL_DAY_MM', 'CLOUDCOVER_AVG_PERCENT', 'SUNHOUR', 'VISIBILITY_AVG_KM']]\n",
    "\n",
    "# Afficher les premières lignes du nouveau DataFrame pour vérification\n",
    "print(df_meteo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arrondir les valeures des champs du csv meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DATE  CLOUDCOVER_AVG_PERCENT  SUNHOUR\n",
      "0     2021-01-01                      83        3\n",
      "1     2021-01-02                      55        9\n",
      "2     2021-01-03                      71        9\n",
      "3     2021-01-04                     100        3\n",
      "4     2021-01-05                      98        3\n",
      "...          ...                     ...      ...\n",
      "1090  2023-12-27                      42        7\n",
      "1091  2023-12-28                      61        5\n",
      "1092  2023-12-29                      86        1\n",
      "1093  2023-12-30                      55        5\n",
      "1094  2023-12-31                      81        3\n",
      "\n",
      "[1095 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "new_data = []\n",
    "\n",
    "# Itérer sur les lignes du DataFrame original\n",
    "for index, row in df_meteo.iterrows():\n",
    "    # Extraire la première colonne (DATE)\n",
    "    date_value = row['DATE']\n",
    "    \n",
    "    # Arrondir les valeurs des trois dernières colonnes\n",
    "    precip_rounded = round(row['PRECIP_TOTAL_DAY_MM'])\n",
    "    cloudcover_rounded = round(row['CLOUDCOVER_AVG_PERCENT'])\n",
    "    sunhour_rounded = round(row['SUNHOUR'])\n",
    "    visibility_rounded = round(row['VISIBILITY_AVG_KM'])\n",
    "    \n",
    "    # Ajouter les valeurs arrondies et la date à la nouvelle ligne de données\n",
    "    new_row = [date_value, precip_rounded, cloudcover_rounded, sunhour_rounded, visibility_rounded]\n",
    "    \n",
    "    # Ajouter la nouvelle ligne au DataFrame\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Créer un nouveau DataFrame avec les données arrondies\n",
    "df_meteo = pd.DataFrame(new_data, columns=['DATE', 'PRECIP_TOTAL_DAY_MM', 'CLOUDCOVER_AVG_PERCENT', 'SUNHOUR', 'VISIBILITY_AVG_KM'])\n",
    "\n",
    "# Afficher les premières lignes du nouveau DataFrame pour vérification\n",
    "\n",
    "df_meteo= df_meteo[['DATE', 'CLOUDCOVER_AVG_PERCENT', 'SUNHOUR']]\n",
    "print(df_meteo.head(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo.to_csv('df_meteo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enlever eventuels doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo = df_meteo.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "UNIQUE constraint failed: lumiere.date",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m curseur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m dfg\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     curseur\u001b[39m.\u001b[39;49mexecute(\u001b[39m\"\u001b[39;49m\u001b[39mINSERT INTO lumiere (date, cloud, sun) VALUES (?, ?, ?)\u001b[39;49m\u001b[39m\"\u001b[39;49m, (row[\u001b[39m'\u001b[39;49m\u001b[39mDATE\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mCLOUDCOVER_AVG_PERCENT\u001b[39;49m\u001b[39m'\u001b[39;49m], (row[\u001b[39m'\u001b[39;49m\u001b[39mSUNHOUR\u001b[39;49m\u001b[39m'\u001b[39;49m])))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m conn\u001b[39m.\u001b[39mcommit()   \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# for index, row in dif.iterrows():\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#     curseur.execute(\"INSERT INTO gare (nom_gare, frequent_2019,frequent_2020,frequent_2021,frequent_2022) VALUES (?, ?,?,?,?)\", (row['gare'], row['frequent_2019'],row['frequent_2020'],row['frequent_2021'],row['frequent_2022']))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# conn.commit()    \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# for index, row in daf.iterrows():\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#     curseur.execute(\"INSERT INTO objets_trouves (data,typo,gare) VALUES (?, ?,?)\", (row['data'], row['typo'],row['gare']))\u001b[39;00m\n",
      "\u001b[0;31mIntegrityError\u001b[0m: UNIQUE constraint failed: lumiere.date"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "\n",
    "dif = pd.read_csv('frequentation.csv')\n",
    "\n",
    "conn = sqlite3.connect('bddt.db')\n",
    "curseur = conn.cursor()\n",
    "\n",
    "for index, row in dfg.iterrows():\n",
    "    curseur.execute(\"INSERT INTO lumiere (date, cloud, sun) VALUES (?, ?, ?)\", (row['DATE'], row['CLOUDCOVER_AVG_PERCENT'], (row['SUNHOUR'])))\n",
    "conn.commit()   \n",
    "# for index, row in dif.iterrows():\n",
    "#     curseur.execute(\"INSERT INTO gare (nom_gare, frequent_2019,frequent_2020,frequent_2021,frequent_2022) VALUES (?, ?,?,?,?)\", (row['gare'], row['frequent_2019'],row['frequent_2020'],row['frequent_2021'],row['frequent_2022']))\n",
    "# conn.commit()    \n",
    "\n",
    "# for index, row in daf.iterrows():\n",
    "#     curseur.execute(\"INSERT INTO objets_trouves (data,typo,gare) VALUES (?, ?,?)\", (row['data'], row['typo'],row['gare']))\n",
    "conn.commit()    \n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# garder 10 premiers caractéres des lignes date \"objets trouvés\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('bddG.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Sélection des données de la table dfg\n",
    "cursor.execute(\"SELECT data FROM objets_trouves\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Mise à jour des données avec les 10 premiers caractères\n",
    "updated_data = [(data[:10],) for (data,) in rows]\n",
    "\n",
    "# Mise à jour de la table dfg avec les nouvelles données\n",
    "cursor.executemany(\"UPDATE objets_trouves SET data = ?\", updated_data)\n",
    "\n",
    "# Validation des changements\n",
    "conn.commit()\n",
    "\n",
    "# Fermeture de la connexion\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes du fichier CSV:\n",
      "Index(['DATE,MAX_TEMPERATURE_C,MIN_TEMPERATURE_C,WINDSPEED_MAX_KMH,TEMPERATURE_MORNING_C,TEMPERATURE_NOON_C,TEMPERATURE_EVENING_C,PRECIP_TOTAL_DAY_MM,HUMIDITY_MAX_PERCENT,VISIBILITY_AVG_KM,PRESSURE_MAX_MB,CLOUDCOVER_AVG_PERCENT,HEATINDEX_MAX_C,DEWPOINT_MAX_C,WINDTEMP_MAX_C,WEATHER_CODE_MORNING,WEATHER_CODE_NOON,WEATHER_CODE_EVENING,TOTAL_SNOW_MM,UV_INDEX,SUNHOUR,OPINION,SUNSET,SUNRISE,TEMPERATURE_NIGHT_C'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lire le fichier CSV\n",
    "df = pd.read_csv('fichier_concat_good.csv')\n",
    "\n",
    "# Afficher les noms des colonnes\n",
    "print(\"Colonnes du fichier CSV:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tri alphabetique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DATE,MAX_TEMPERATURE_C,MIN_TEMPERATURE_C,WINDSPEED_MAX_KMH,TEMPERATURE_MORNING_C,TEMPERATURE_NOON_C,TEMPERATURE_EVENING_C,PRECIP_TOTAL_DAY_MM,HUMIDITY_MAX_PERCENT,VISIBILITY_AVG_KM,PRESSURE_MAX_MB,CLOUDCOVER_AVG_PERCENT,HEATINDEX_MAX_C,DEWPOINT_MAX_C,WINDTEMP_MAX_C,WEATHER_CODE_MORNING,WEATHER_CODE_NOON,WEATHER_CODE_EVENING,TOTAL_SNOW_MM,UV_INDEX,SUNHOUR,OPINION,SUNSET,SUNRISE,TEMPERATURE_NIGHT_C'], dtype='object')\n",
      "Avant le tri:\n",
      "  DATE,MAX_TEMPERATURE_C,MIN_TEMPERATURE_C,WINDSPEED_MAX_KMH,TEMPERATURE_MORNING_C,TEMPERATURE_NOON_C,TEMPERATURE_EVENING_C,PRECIP_TOTAL_DAY_MM,HUMIDITY_MAX_PERCENT,VISIBILITY_AVG_KM,PRESSURE_MAX_MB,CLOUDCOVER_AVG_PERCENT,HEATINDEX_MAX_C,DEWPOINT_MAX_C,WINDTEMP_MAX_C,WEATHER_CODE_MORNING,WEATHER_CODE_NOON,WEATHER_CODE_EVENING,TOTAL_SNOW_MM,UV_INDEX,SUNHOUR,OPINION,SUNSET,SUNRISE,TEMPERATURE_NIGHT_C\n",
      "0  2021-11-01,15,10,23,11,14,12,0.2,76,10,1005,40...                                                                                                                                                                                                                                                                                                                                                             \n",
      "1  2021-11-02,12,10,22,10,12,10,0.7,84,10,1003,94...                                                                                                                                                                                                                                                                                                                                                             \n",
      "2  2021-11-03,14,7,4,7,12,12,0,79,10,1007,10.75,1...                                                                                                                                                                                                                                                                                                                                                             \n",
      "3  2021-11-04,12,6,19,6,11,10,0.3,82,10,1019,48.1...                                                                                                                                                                                                                                                                                                                                                             \n",
      "4  2021-11-05,12,7,11,8,11,11,0,78,10,1029,60.625...                                                                                                                                                                                                                                                                                                                                                             \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6902/2514206105.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Avant le tri:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Trier le DataFrame par la colonne de dates en utilisant les dates comme chaînes de caractères\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DATE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Enregistrer le DataFrame trié dans un nouveau fichier CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fichier_concat_good_actu4.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7185\u001b[0m             )\n\u001b[1;32m   7186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7187\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7189\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7191\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7192\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV\n",
    "file_path = 'fichier_concat_good_actu.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.columns)\n",
    "# Afficher les premières lignes pour vérifier les données\n",
    "print(\"Avant le tri:\")\n",
    "print(df.head())\n",
    "\n",
    "# Trier le DataFrame par la colonne de dates en utilisant les dates comme chaînes de caractères\n",
    "df_sorted = df.sort_values(by='DATE')\n",
    "\n",
    "# Enregistrer le DataFrame trié dans un nouveau fichier CSV\n",
    "output_path = 'fichier_concat_good_actu4.csv'\n",
    "df_sorted.to_csv(output_path, index=False)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame trié\n",
    "print(\"Après le tri:\")\n",
    "print(df_sorted.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convertir date en datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DATE,MAX_TEMPERATURE_C,MIN_TEMPERATURE_C,WINDSPEED_MAX_KMH,TEMPERATURE_MORNING_C,TEMPERATURE_NOON_C,TEMPERATURE_EVENING_C,PRECIP_TOTAL_DAY_MM,HUMIDITY_MAX_PERCENT,VISIBILITY_AVG_KM,PRESSURE_MAX_MB,CLOUDCOVER_AVG_PERCENT,HEATINDEX_MAX_C,DEWPOINT_MAX_C,WINDTEMP_MAX_C,WEATHER_CODE_MORNING,WEATHER_CODE_NOON,WEATHER_CODE_EVENING,TOTAL_SNOW_MM,UV_INDEX,SUNHOUR,OPINION,SUNSET,SUNRISE,TEMPERATURE_NIGHT_C\n",
      "0  2021-11-01,15,10,23,11,14,12,0.2,76,10,1005,40...                                                                                                                                                                                                                                                                                                                                                             \n",
      "1  2021-11-02,12,10,22,10,12,10,0.7,84,10,1003,94...                                                                                                                                                                                                                                                                                                                                                             \n",
      "2  2021-11-03,14,7,4,7,12,12,0,79,10,1007,10.75,1...                                                                                                                                                                                                                                                                                                                                                             \n",
      "3  2021-11-04,12,6,19,6,11,10,0.3,82,10,1019,48.1...                                                                                                                                                                                                                                                                                                                                                             \n",
      "4  2021-11-05,12,7,11,8,11,11,0,78,10,1029,60.625...                                                                                                                                                                                                                                                                                                                                                             \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DATE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mhead())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Convertir la colonne de dates en datetime avec le format spécifique\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mdate_column\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39;49m\u001b[39mDATE\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Trier le DataFrame par la colonne de dates\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/operation_good_concat.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df_sorted \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdate_column\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DATE'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV\n",
    "file_path = 'fichier_concat_good_actu.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Afficher les premières lignes pour vérifier les données\n",
    "print(df.head())\n",
    "\n",
    "# Convertir la colonne de dates en datetime avec le format spécifique\n",
    "df['date_column'] = pd.to_datetime(df['DATE'], format='%Y-%d-%m')\n",
    "\n",
    "# Trier le DataFrame par la colonne de dates\n",
    "df_sorted = df.sort_values(by='date_column')\n",
    "\n",
    "# Enregistrer le DataFrame trié dans un nouveau fichier CSV\n",
    "output_path = 'fichier_concat_good_actu4.csv'\n",
    "df_sorted.to_csv(output_path, index=False)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame trié\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DATE,MAX_TEMPERATURE_C,MIN_TEMPERATURE_C,WINDSPEED_MAX_KMH,TEMPERATURE_MORNING_C,TEMPERATURE_NOON_C,TEMPERATURE_EVENING_C,PRECIP_TOTAL_DAY_MM,HUMIDITY_MAX_PERCENT,VISIBILITY_AVG_KM,PRESSURE_MAX_MB,CLOUDCOVER_AVG_PERCENT,HEATINDEX_MAX_C,DEWPOINT_MAX_C,WINDTEMP_MAX_C,WEATHER_CODE_MORNING,WEATHER_CODE_NOON,WEATHER_CODE_EVENING,TOTAL_SNOW_MM,UV_INDEX,SUNHOUR,OPINION,SUNSET,SUNRISE,TEMPERATURE_NIGHT_C'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'fichier_concat_good_actu.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fichier_concat_good_actu.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb Cell 19\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msqlite3\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Charger le fichier CSV\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mfichier_concat_good_actu.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Afficher les colonnes pour vérifier les noms\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/utilisateur/Documents/all_projects/rendu_sncf/lost_translation_sncf/lancement_script/pipeline_good_concat.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mColonnes du DataFrame :\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/api/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/api/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/api/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/api/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/api/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fichier_concat_good_actu.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df = pd.read_csv(\"fichier_concat_good_actu.csv\")\n",
    "\n",
    "# Afficher les colonnes pour vérifier les noms\n",
    "print(\"Colonnes du DataFrame :\")\n",
    "print(df.columns)\n",
    "\n",
    "# Connexion à la base de données SQLite\n",
    "conn = sqlite3.connect('bdd_luz.db')\n",
    "curseur = conn.cursor()\n",
    "\n",
    "# Itérer sur les lignes du DataFrame et insérer dans la base de données\n",
    "n = 1\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        curseur.execute(\n",
    "            \"INSERT INTO lumiere (DATE, cloud, sun) VALUES (?, ?, ?)\",\n",
    "            (row['DATE'], row['CLOUDCOVER_AVG_PERCENT'], row['SUNHOUR'])\n",
    "        )\n",
    "        print(f\"Inséré ligne {n}\")\n",
    "        n += 1\n",
    "    except KeyError as e:\n",
    "        print(f\"Erreur de clé : {e}\")\n",
    "\n",
    "# Valider les changements et fermer la connexion\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# poids de la fréquentation pour chaque gare (pour comparer nombre objets trouvés)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rajout de ligne des frequentations totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                     gare  frequent_2021  \\\n",
      "0         0.0                       Paris Gare de Lyon      189125448   \n",
      "1         1.0                       Paris Montparnasse       92442771   \n",
      "2         2.0                       Paris Gare du Nord      275838875   \n",
      "3         3.0                                Paris Est       62800592   \n",
      "4         4.0                       Paris Saint-Lazare      213505115   \n",
      "5         5.0                         Paris Austerlitz       45096165   \n",
      "6         6.0  Paris Bercy Bourgogne - Pays d'Auvergne        6821651   \n",
      "7         NaN                              Paris total      885630617   \n",
      "\n",
      "   frequent_2022  frequent_2023  \n",
      "0      231360748      251046517  \n",
      "1      128082169      145228277  \n",
      "2      451647681      482925909  \n",
      "3       83197515       90216815  \n",
      "4      240124708      241391780  \n",
      "5       39824989       44259883  \n",
      "6        9710201       10340177  \n",
      "7     1183948011     1265409358  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"frequentation_actu.csv\")\n",
    "\n",
    "# Calculer les totaux pour chaque colonne de fréquentation\n",
    "total_frequent_2021 = df[\"frequent_2021\"].sum()\n",
    "total_frequent_2022 = df[\"frequent_2022\"].sum()\n",
    "total_frequent_2023 = df[\"frequent_2023\"].sum()\n",
    "\n",
    "# Créer une nouvelle ligne avec les totaux\n",
    "total_row = pd.DataFrame({\n",
    "    \"gare\": [\"Paris total\"],\n",
    "    \"frequent_2021\": [total_frequent_2021],\n",
    "    \"frequent_2022\": [total_frequent_2022],\n",
    "    \"frequent_2023\": [total_frequent_2023]\n",
    "})\n",
    "\n",
    "# Ajouter cette ligne à la fin du DataFrame\n",
    "df = pd.concat([df, total_row], ignore_index=True)\n",
    "\n",
    "# Réécrire le DataFrame dans le fichier CSV\n",
    "df.to_csv(\"frequentation_actu.csv\", index=False)\n",
    "\n",
    "# Afficher le DataFrame pour vérification\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calcul coefficient pour chaque gare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                     gare  frequent_2021  \\\n",
      "0         0.0                       Paris Gare de Lyon      189125448   \n",
      "1         1.0                       Paris Montparnasse       92442771   \n",
      "2         2.0                       Paris Gare du Nord      275838875   \n",
      "3         3.0                                Paris Est       62800592   \n",
      "4         4.0                       Paris Saint-Lazare      213505115   \n",
      "5         5.0                         Paris Austerlitz       45096165   \n",
      "6         6.0  Paris Bercy Bourgogne - Pays d'Auvergne        6821651   \n",
      "7         NaN                              Paris total      885630617   \n",
      "\n",
      "   frequent_2022  frequent_2023  percent_2021  percent_2022  percent_2023  \n",
      "0      231360748      251046517        0.2135        0.1954        0.1984  \n",
      "1      128082169      145228277        0.1044        0.1082        0.1148  \n",
      "2      451647681      482925909        0.3115        0.3815        0.3816  \n",
      "3       83197515       90216815        0.0709        0.0703        0.0713  \n",
      "4      240124708      241391780        0.2411        0.2028        0.1908  \n",
      "5       39824989       44259883        0.0509        0.0336        0.0350  \n",
      "6        9710201       10340177        0.0077        0.0082        0.0082  \n",
      "7     1183948011     1265409358        1.0000        1.0000        1.0000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV dans un DataFrame\n",
    "df = pd.read_csv(\"frequentation_actu.csv\")\n",
    "\n",
    "# Assurer que la ligne \"Paris total\" est bien la dernière\n",
    "paris_total = df[df['gare'] == 'Paris total']\n",
    "\n",
    "# Calculer les totaux pour chaque année (dernière ligne 'Paris total')\n",
    "total_frequent_2021 = paris_total['frequent_2021'].values[0]\n",
    "total_frequent_2022 = paris_total['frequent_2022'].values[0]\n",
    "total_frequent_2023 = paris_total['frequent_2023'].values[0]\n",
    "\n",
    "# Calculer les pourcentages pour chaque gare par rapport au total de l'année\n",
    "df['percent_2021'] = (df['frequent_2021'] / total_frequent_2021) * 100\n",
    "df['percent_2022'] = (df['frequent_2022'] / total_frequent_2022) * 100\n",
    "df['percent_2023'] = (df['frequent_2023'] / total_frequent_2023) * 100\n",
    "\n",
    "# Formater les pourcentages comme demandé (exemple : 1,16 pour 16%)\n",
    "df['percent_2021'] = df['percent_2021'].apply(lambda x: round(x, 2) / 100)\n",
    "df['percent_2022'] = df['percent_2022'].apply(lambda x: round(x, 2) / 100)\n",
    "df['percent_2023'] = df['percent_2023'].apply(lambda x: round(x, 2) / 100)\n",
    "\n",
    "# Garder la ligne \"Paris total\" intacte avec des valeurs de 1,00 (100%) dans les colonnes de pourcentage\n",
    "df.loc[df['gare'] == 'Paris total', ['percent_2021', 'percent_2022', 'percent_2023']] = 1.00\n",
    "\n",
    "# Réécrire le DataFrame dans le fichier CSV\n",
    "df.to_csv(\"frequentation_actu.csv\", index=False)\n",
    "\n",
    "# Afficher le DataFrame pour vérification\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rajouter une colonne toutes frequentations (qui reprend années 2021, 2022 ,2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                     gare  frequent_2021  \\\n",
      "0         0.0                       Paris Gare de Lyon      189125448   \n",
      "1         1.0                       Paris Montparnasse       92442771   \n",
      "2         2.0                       Paris Gare du Nord      275838875   \n",
      "3         3.0                                Paris Est       62800592   \n",
      "4         4.0                       Paris Saint-Lazare      213505115   \n",
      "5         5.0                         Paris Austerlitz       45096165   \n",
      "6         6.0  Paris Bercy Bourgogne - Pays d'Auvergne        6821651   \n",
      "7         NaN                              Paris total      885630617   \n",
      "\n",
      "   frequent_2022  frequent_2023  percent_2021  percent_2022  percent_2023  \\\n",
      "0      231360748      251046517        0.2135        0.1954        0.1984   \n",
      "1      128082169      145228277        0.1044        0.1082        0.1148   \n",
      "2      451647681      482925909        0.3115        0.3815        0.3816   \n",
      "3       83197515       90216815        0.0709        0.0703        0.0713   \n",
      "4      240124708      241391780        0.2411        0.2028        0.1908   \n",
      "5       39824989       44259883        0.0509        0.0336        0.0350   \n",
      "6        9710201       10340177        0.0077        0.0082        0.0082   \n",
      "7     1183948011     1265409358        1.0000        1.0000        1.0000   \n",
      "\n",
      "   total_frequentations  percent_total  \n",
      "0             671532713         0.2014  \n",
      "1             365753217         0.1097  \n",
      "2            1210412465         0.3629  \n",
      "3             236214922         0.0708  \n",
      "4             695021603         0.2084  \n",
      "5             129181037         0.0387  \n",
      "6              26872029         0.0081  \n",
      "7            3334987986         1.0000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV dans un DataFrame\n",
    "df = pd.read_csv(\"frequentation_actu.csv\")\n",
    "\n",
    "# Créer la colonne 'total_frequentations' en additionnant les valeurs des colonnes 2021, 2022 et 2023, si ce n'est pas déjà fait\n",
    "if 'total_frequentations' not in df.columns:\n",
    "    df['total_frequentations'] = df['frequent_2021'] + df['frequent_2022'] + df['frequent_2023']\n",
    "\n",
    "# Trouver le total des fréquentations pour \"Paris total\"\n",
    "paris_total_frequentations = df.loc[df['gare'] == 'Paris total', 'total_frequentations'].values[0]\n",
    "\n",
    "# Recalculer la colonne percent_total pour chaque gare\n",
    "df['percent_total'] = (df['total_frequentations'] / paris_total_frequentations) * 100\n",
    "\n",
    "# Formater les pourcentages comme demandé (exemple : 1,16 pour 16%)\n",
    "df['percent_total'] = df['percent_total'].apply(lambda x: round(x, 2) / 100)\n",
    "\n",
    "# Garder la ligne \"Paris total\" intacte avec une valeur de 1.00 (100%)\n",
    "df.loc[df['gare'] == 'Paris total', 'percent_total'] = 1.00\n",
    "\n",
    "# Réécrire le DataFrame dans le fichier CSV\n",
    "df.to_csv(\"frequentation_actu.csv\", index=False)\n",
    "\n",
    "# Afficher le DataFrame pour vérification\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_chef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
